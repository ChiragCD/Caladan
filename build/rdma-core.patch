diff --git a/providers/mlx5/dr_domain.c b/providers/mlx5/dr_domain.c
index 53d9decb..612969fe 100644
--- a/providers/mlx5/dr_domain.c
+++ b/providers/mlx5/dr_domain.c
@@ -307,9 +307,9 @@ static int dr_domain_caps_init(struct ibv_context *ctx,
 	/* Non FDB type is supported over root table or when we can enable
 	 * force-loopback.
 	 */
-	if ((dmn->type != MLX5DV_DR_DOMAIN_TYPE_FDB) &&
-	    !dr_send_allow_fl(&dmn->info.caps))
-		return 0;
+//	if ((dmn->type != MLX5DV_DR_DOMAIN_TYPE_FDB) &&
+//	    !dr_send_allow_fl(&dmn->info.caps))
+//		return 0;
 
 	ret = dr_domain_vports_init(dmn);
 	if (ret)
@@ -470,6 +470,7 @@ mlx5dv_dr_domain_create(struct ibv_context *ctx,
 	dmn->type = type;
 	atomic_init(&dmn->refcount, 1);
 	list_head_init(&dmn->tbl_list);
+	dmn->spinlock = 0;
 
 	ret = pthread_spin_init(&dmn->debug_lock, PTHREAD_PROCESS_PRIVATE);
 	if (ret) {
diff --git a/providers/mlx5/dr_rule.c b/providers/mlx5/dr_rule.c
index ccba6421..abcb53bd 100644
--- a/providers/mlx5/dr_rule.c
+++ b/providers/mlx5/dr_rule.c
@@ -32,8 +32,11 @@
 
 #include <stdlib.h>
 #include <ccan/minmax.h>
+#include "mlx5dv.h"
 #include "mlx5dv_dr.h"
 
+#include "dr_ste.h"
+
 /* +1 for the cross GVMI STE */
 #define DR_RULE_MAX_STE_CHAIN (DR_RULE_MAX_STES + DR_ACTION_MAX_STES + 1)
 
@@ -1630,6 +1633,36 @@ struct mlx5dv_dr_rule *mlx5dv_dr_rule_create(struct mlx5dv_dr_matcher *matcher,
 	return rule;
 }
 
+
+int switch_qp_action(struct mlx5dv_dr_rule *rule,
+	struct mlx5dv_dr_domain *dmn,
+	struct ibv_qp *nqp, struct ibv_qp *pqp)
+{
+	uint64_t old_qp_index;
+	struct dr_ste *ste = &rule->rx.nic_matcher->s_htbl->ste_arr[0];
+	struct mlx5_qp *next_qp = to_mqp(nqp);
+	struct mlx5_qp *prev_qp = to_mqp(pqp);
+	struct postsend_info send_info = {};
+
+	assert(dmn->spinlock == 1);
+
+	assert(ste->htbl->chunk->num_of_entries == 1);
+
+	old_qp_index = dmn->ste_ctx->get_hit_addr(ste->hw_ste) & ~0x1;
+	assert(old_qp_index == ((prev_qp->tir_icm_addr >> 5) & 0xffffffff));
+
+	dr_ste_set_hit_addr(dmn->ste_ctx, ste->hw_ste, next_qp->tir_icm_addr, 1);
+
+	send_info.write.addr    = (uintptr_t) ste->hw_ste;
+	send_info.write.length  = DR_STE_SIZE_REDUCED;
+	send_info.write.lkey    = 0;
+	send_info.remote_addr   = dr_ste_get_mr_addr(ste);
+	send_info.rkey          = ste->htbl->chunk->rkey;
+
+	return dr_postsend_icm_data_unlocked(dmn, &send_info, 0);
+}
+
+
 int mlx5dv_dr_rule_destroy(struct mlx5dv_dr_rule *rule)
 {
 	struct mlx5dv_dr_matcher *matcher = rule->matcher;
diff --git a/providers/mlx5/dr_send.c b/providers/mlx5/dr_send.c
index 16d69150..fa4155d5 100644
--- a/providers/mlx5/dr_send.c
+++ b/providers/mlx5/dr_send.c
@@ -39,7 +39,7 @@
 
 #define QUEUE_SIZE		128
 #define SIGNAL_PER_DIV_QUEUE	16
-#define TH_NUMS_TO_DRAIN	2
+#define TH_NUMS_TO_DRAIN	16
 
 enum {
 	CQ_OK = 0,
@@ -747,7 +747,7 @@ static void dr_fill_data_segs(struct mlx5dv_dr_domain *dmn,
 		dr_fill_write_args_segs(send_ring, send_info);
 }
 
-static int dr_postsend_icm_data(struct mlx5dv_dr_domain *dmn,
+int dr_postsend_icm_data_unlocked(struct mlx5dv_dr_domain *dmn,
 				struct postsend_info *send_info,
 				int ring_idx)
 {
@@ -768,6 +768,31 @@ out_unlock:
 	return ret;
 }
 
+void postsend_lock(struct mlx5dv_dr_domain *dmn)
+{
+	while (__sync_lock_test_and_set(&dmn->spinlock, 1)) {
+		while (dmn->spinlock)
+			asm volatile("pause");
+	}
+}
+
+void postsend_unlock(struct mlx5dv_dr_domain *dmn)
+{
+	__sync_lock_release(&dmn->spinlock);
+}
+
+static int dr_postsend_icm_data(struct mlx5dv_dr_domain *dmn,
+				struct postsend_info *send_info, int ring_idx)
+{
+	int ret;
+
+	postsend_lock(dmn);
+	ret = dr_postsend_icm_data_unlocked(dmn, send_info, ring_idx);
+	postsend_unlock(dmn);
+
+	return ret;
+}
+
 static int dr_get_tbl_copy_details(struct mlx5dv_dr_domain *dmn,
 				   struct dr_ste_htbl *htbl,
 				   uint8_t **data,
diff --git a/providers/mlx5/dr_ste.h b/providers/mlx5/dr_ste.h
index 832e2ca1..8b46beb5 100644
--- a/providers/mlx5/dr_ste.h
+++ b/providers/mlx5/dr_ste.h
@@ -208,6 +208,7 @@ struct dr_ste_ctx {
 	void (*set_miss_addr)(uint8_t *hw_ste_p, uint64_t miss_addr);
 	uint64_t (*get_miss_addr)(uint8_t *hw_ste_p);
 	void (*set_hit_addr)(uint8_t *hw_ste_p, uint64_t icm_addr, uint32_t ht_size);
+	uint64_t (*get_hit_addr)(uint8_t *hw_ste_p);
 	void (*set_byte_mask)(uint8_t *hw_ste_p, uint16_t byte_mask);
 	uint16_t (*get_byte_mask)(uint8_t *hw_ste_p);
 	void (*set_ctrl_always_hit_htbl)(uint8_t *hw_ste, uint16_t byte_mask,
diff --git a/providers/mlx5/dr_ste_v0.c b/providers/mlx5/dr_ste_v0.c
index b9a23ac0..af746fc9 100644
--- a/providers/mlx5/dr_ste_v0.c
+++ b/providers/mlx5/dr_ste_v0.c
@@ -341,6 +341,15 @@ static void dr_ste_v0_init_full(uint8_t *hw_ste_p, uint16_t lu_type,
 	DR_STE_SET(rx_steering_mult, hw_ste_p, miss_address_63_48, gvmi);
 }
 
+static uint64_t dr_ste_v0_get_hit_addr(uint8_t *hw_ste_p)
+{
+	uint64_t index = DR_STE_GET(general, hw_ste_p, next_table_base_31_5_size) |
+			DR_STE_GET(general, hw_ste_p, next_table_base_39_32_size) << 27;
+
+	return index;
+
+}
+
 static void dr_ste_v0_init(uint8_t *hw_ste_p, uint16_t lu_type,
 			   bool is_rx, uint16_t gvmi)
 {
@@ -1908,6 +1917,7 @@ static struct dr_ste_ctx ste_ctx_v0 = {
 	.set_miss_addr			= &dr_ste_v0_set_miss_addr,
 	.get_miss_addr			= &dr_ste_v0_get_miss_addr,
 	.set_hit_addr			= &dr_ste_v0_set_hit_addr,
+	.get_hit_addr			= &dr_ste_v0_get_hit_addr,
 	.set_byte_mask			= &dr_ste_v0_set_byte_mask,
 	.get_byte_mask			= &dr_ste_v0_get_byte_mask,
 	.set_ctrl_always_hit_htbl	= &dr_ste_v0_set_ctrl_always_hit_htbl,
diff --git a/providers/mlx5/dr_ste_v1.c b/providers/mlx5/dr_ste_v1.c
index 0bc8b5d9..00efb0ce 100644
--- a/providers/mlx5/dr_ste_v1.c
+++ b/providers/mlx5/dr_ste_v1.c
@@ -391,6 +391,13 @@ static bool dr_ste_v1_is_match_ste(uint16_t lu_type)
 	return ((lu_type >> 8) == DR_STE_V1_TYPE_MATCH);
 }
 
+static uint64_t dr_ste_v1_get_hit_addr(uint8_t *hw_ste_p)
+{
+	uint64_t index = DR_STE_GET(match_bwc_v1, hw_ste_p, next_table_base_31_5_size) |
+			DR_STE_GET(match_bwc_v1, hw_ste_p, next_table_base_39_32_size) << 27;
+	return index;
+}
+
 static void dr_ste_v1_init(uint8_t *hw_ste_p, uint16_t lu_type,
 			   bool is_rx, uint16_t gvmi)
 {
@@ -3589,6 +3596,7 @@ static struct dr_ste_ctx ste_ctx_v1 = {
 	.set_miss_addr			= &dr_ste_v1_set_miss_addr,
 	.get_miss_addr			= &dr_ste_v1_get_miss_addr,
 	.set_hit_addr			= &dr_ste_v1_set_hit_addr,
+	.get_hit_addr 			= &dr_ste_v1_get_hit_addr,
 	.set_byte_mask			= &dr_ste_v1_set_byte_mask,
 	.get_byte_mask			= &dr_ste_v1_get_byte_mask,
 	.set_ctrl_always_hit_htbl	= &dr_ste_v1_set_ctrl_always_hit_htbl,
diff --git a/providers/mlx5/libmlx5.map b/providers/mlx5/libmlx5.map
index 8eeb261b..90b6e604 100644
--- a/providers/mlx5/libmlx5.map
+++ b/providers/mlx5/libmlx5.map
@@ -105,6 +105,9 @@ MLX5_1.10 {
 		mlx5dv_dr_table_create;
 		mlx5dv_dr_table_destroy;
 		mlx5dv_qp_ex_from_ibv_qp_ex;
+		switch_qp_action;
+		postsend_lock;
+		postsend_unlock;
 } MLX5_1.9;
 
 MLX5_1.11 {
@@ -228,4 +231,11 @@ MLX5_1.24 {
 		mlx5dv_crypto_login_query;
 		mlx5dv_destroy_steering_anchor;
 		mlx5dv_dr_action_create_dest_root_table;
+		export_fd;
+		mlx5_vfio_post_cmd_db;
+		mlx5_vfio_post_cmd_fast;
+		mlx5_vfio_cmd_slot_alloc;
+		mlx5_vfio_cmd_slot_get_fd;
+		mlx5_vfio_get_clock;
+		mlx5_access_reg;
 } MLX5_1.23;
diff --git a/providers/mlx5/mlx5_vfio.c b/providers/mlx5/mlx5_vfio.c
index fa19f2a9..658315ad 100644
--- a/providers/mlx5/mlx5_vfio.c
+++ b/providers/mlx5/mlx5_vfio.c
@@ -524,7 +524,7 @@ static int mlx5_vfio_wait_event(struct mlx5_vfio_context *ctx,
 
 	while (true) {
 		err = poll(fds, 2, -1);
-		if (err < 0 && errno != EAGAIN) {
+		if (err < 0 && errno != EAGAIN && errno != EINTR) {
 			mlx5_err(ctx->dbg_fp, "mlx5_vfio_wait_event, poll failed, errno=%d\n", errno);
 			return errno;
 		}
@@ -599,28 +599,105 @@ static int mlx5_vfio_cmd_prep_out(struct mlx5_vfio_context *ctx,
 				  struct mlx5_cmd_msg *cmd_out,
 				  struct mlx5_cmd_layout *cmd_lay, int olen)
 {
-	struct mlx5_cmd_mailbox *tmp;
-	struct mlx5_cmd_block *block;
+	// struct mlx5_cmd_mailbox *tmp;
+	// struct mlx5_cmd_block *block;
 
 	cmd_lay->olen = htobe32(olen);
 
 	/* zeroing output header */
-	memset(cmd_lay->out, 0, sizeof(cmd_lay->out));
+	// memset(cmd_lay->out, 0, sizeof(cmd_lay->out));
 
 	if (olen > cmd_out->len)
 		/* Upon enlarge output message is zeroed */
 		return mlx5_vfio_enlarge_cmd_msg(ctx, cmd_out, cmd_lay, olen, false);
 
 	/* zeroing output message */
-	tmp = cmd_out->next;
-	olen -= min_t(int, olen, sizeof(cmd_lay->out));
-	while (olen > 0) {
-		block = tmp->buf;
-		memset(block->data, 0, MLX5_CMD_DATA_BLOCK_SIZE);
-		olen -= MLX5_CMD_DATA_BLOCK_SIZE;
-		tmp = tmp->next;
-		assert(tmp || olen <= 0);
-	}
+	// tmp = cmd_out->next;
+	// olen -= min_t(int, olen, sizeof(cmd_lay->out));
+	// while (olen > 0) {
+	// 	block = tmp->buf;
+	// 	memset(block->data, 0, MLX5_CMD_DATA_BLOCK_SIZE);
+	// 	olen -= MLX5_CMD_DATA_BLOCK_SIZE;
+	// 	tmp = tmp->next;
+	// 	assert(tmp || olen <= 0);
+	// }
+	return 0;
+}
+
+void mlx5_vfio_post_cmd_db(struct ibv_context *ibctx, unsigned int vector)
+{
+	struct mlx5_vfio_context *ctx = to_mvfio_ctx(ibctx);
+	struct mlx5_init_seg *init_seg = ctx->bar_map;
+
+	udma_to_device_barrier();
+	mmio_write32_be(&init_seg->cmd_dbell, htobe32(vector));
+}
+
+int mlx5_vfio_post_cmd_fast(struct ibv_context *ibctx, void *in,
+			      int ilen, int olen,
+			      unsigned int slot)
+{
+	struct mlx5_vfio_context *ctx = to_mvfio_ctx(ibctx);
+	struct mlx5_cmd_layout *cmd_lay = ctx->cmd.cmds[slot].lay;
+	struct mlx5_cmd_msg *cmd_in = &ctx->cmd.cmds[slot].in;
+	struct mlx5_cmd_msg *cmd_out = &ctx->cmd.cmds[slot].out;
+
+	int err;
+
+	err = mlx5_vfio_cmd_prep_in(ctx, cmd_in, cmd_lay, in, ilen);
+	if (err)
+		return err;
+
+	err = mlx5_vfio_cmd_prep_out(ctx, cmd_out, cmd_lay, olen);
+	if (err)
+		return err;
+
+	cmd_lay->status_own = 0x1;
+	return 0;
+}
+
+static int mlx5_vfio_setup_cmd_slot(struct mlx5_vfio_context *ctx, int slot);
+
+int mlx5_vfio_get_clock(struct ibv_context *ibctx, unsigned int *frequency_khz, void **core_clock)
+{
+	struct mlx5_vfio_context *ctx = to_mvfio_ctx(ibctx);
+
+	*frequency_khz = MLX5_VFIO_CAP_GEN(ctx, device_frequency_khz);
+	*core_clock = &ctx->bar_map->internal_timer_h;
+
+	return 0;
+}
+
+
+int mlx5_vfio_cmd_slot_get_fd(struct ibv_context *ibctx, uint32_t slot)
+{
+	struct mlx5_vfio_context *ctx = to_mvfio_ctx(ibctx);
+	struct mlx5_vfio_cmd_slot *cmd_slot;
+
+	if (slot >= MLX5_MAX_COMMANDS)
+		return -EINVAL;
+
+	return ctx->cmd.cmds[slot].completion_event_fd;
+}
+
+
+int mlx5_vfio_cmd_slot_alloc(struct ibv_context *ibctx, uint32_t *slot_out, struct mlx5_cmd_layout **cmd_lay_out)
+{
+	static uint32_t next_slot = 1;
+	struct mlx5_vfio_context *ctx = to_mvfio_ctx(ibctx);
+	struct mlx5_vfio_cmd_slot *cmd_slot;
+	int ret;
+
+	if (next_slot == MLX5_MAX_COMMANDS - 1)
+		return -ENOSPC;
+
+	ret = mlx5_vfio_setup_cmd_slot(ctx, next_slot);
+	if (ret)
+		return ret;
+
+	*slot_out = next_slot;
+	cmd_slot = &ctx->cmd.cmds[next_slot++];
+	*cmd_lay_out = cmd_slot->lay;
 	return 0;
 }
 
@@ -1131,6 +1208,17 @@ static void mlx5_vfio_uninit_bar0(struct mlx5_vfio_context *ctx)
 	munmap(ctx->bar_map, ctx->bar_map_size);
 }
 
+static uint64_t bar0_offset;
+
+void export_fd(struct ibv_context *ibctx, int *fdout, off_t *offout, size_t *szout)
+{
+	struct mlx5_vfio_context *ctx = to_mvfio_ctx(ibctx);
+
+	*fdout = ctx->device_fd;
+	*szout = ctx->bar_map_size;
+	*offout = bar0_offset;
+}
+
 static int mlx5_vfio_init_bar0(struct mlx5_vfio_context *ctx)
 {
 	struct vfio_region_info reg = { .argsz = sizeof(reg) };
@@ -1147,6 +1235,7 @@ static int mlx5_vfio_init_bar0(struct mlx5_vfio_context *ctx)
 	if (base == MAP_FAILED)
 		return -1;
 
+	bar0_offset = reg.offset;
 	ctx->bar_map = (struct mlx5_init_seg *)base;
 	ctx->bar_map_size = reg.size;
 	return 0;
@@ -1707,6 +1796,17 @@ out:
 	return err;
 }
 
+
+int mlx5_access_reg(struct ibv_context *ibctx, void *data_in,
+				int size_in, void *data_out, int size_out,
+				uint16_t reg_id, int arg, int write)
+{
+	struct mlx5_vfio_context *ctx = to_mvfio_ctx(ibctx);
+
+	return mlx5_vfio_access_reg(ctx, data_in, size_in, data_out, size_out,
+	                            reg_id, arg, write);
+}
+
 static int mlx5_vfio_get_caps_mode(struct mlx5_vfio_context *ctx,
 				   enum mlx5_cap_type cap_type,
 				   enum mlx5_cap_mode cap_mode)
@@ -2592,8 +2692,9 @@ _vfio_devx_umem_reg(struct ibv_context *context,
 
 	iova_page_shift = ilog32(vfio_umem->iova_size - 1);
 	num_pas = 1;
-	if (iova_page_shift > MLX5_MAX_PAGE_SHIFT) {
-		iova_page_shift = MLX5_MAX_PAGE_SHIFT;
+	// Cap at 18, allows 4096 byte boundaries for wqs
+	if (iova_page_shift > 18) {
+		iova_page_shift = 18;
 		num_pas = DIV_ROUND_UP(vfio_umem->iova_size, (1ULL << iova_page_shift));
 	}
 
diff --git a/providers/mlx5/mlx5dv.h b/providers/mlx5/mlx5dv.h
index 163aab7b..5e510aa7 100644
--- a/providers/mlx5/mlx5dv.h
+++ b/providers/mlx5/mlx5dv.h
@@ -1710,6 +1710,20 @@ int mlx5dv_vfio_get_events_fd(struct ibv_context *ibctx);
  */
 int mlx5dv_vfio_process_events(struct ibv_context *context);
 
+
+struct mlx5_cmd_layout;
+extern int mlx5_vfio_cmd_slot_get_fd(struct ibv_context *ibctx, uint32_t slot);
+extern int mlx5_vfio_cmd_slot_alloc(struct ibv_context *ibctx, uint32_t *slot_out, struct mlx5_cmd_layout **cmd_lay_out);
+extern int mlx5_vfio_get_clock(struct ibv_context *ibctx, unsigned int *frequency_khz, void **core_clock);
+extern int mlx5_vfio_post_cmd_fast(struct ibv_context *ibctx, void *in,
+			      int ilen, int olen,
+			      unsigned int slot);
+extern void mlx5_vfio_post_cmd_db(struct ibv_context *ibctx, unsigned int vector);
+extern void export_fd(struct ibv_context *ibctx, int *fdout, off_t *offout, size_t *szout);
+extern int mlx5_access_reg(struct ibv_context *ibctx, void *data_in,
+				int size_in, void *data_out, int size_out,
+				uint16_t reg_id, int arg, int write);
+
 struct ibv_context *
 mlx5dv_open_device(struct ibv_device *device, struct mlx5dv_context_attr *attr);
 
@@ -1999,6 +2013,13 @@ mlx5dv_dr_rule_create(struct mlx5dv_dr_matcher *matcher,
 		      size_t num_actions,
 		      struct mlx5dv_dr_action *actions[]);
 
+
+void postsend_lock(struct mlx5dv_dr_domain *dmn);
+void postsend_unlock(struct mlx5dv_dr_domain *dmn);
+int switch_qp_action(struct mlx5dv_dr_rule *rule,
+	struct mlx5dv_dr_domain *dmn,
+	struct ibv_qp *nqp, struct ibv_qp *pqp);
+
 int mlx5dv_dr_rule_destroy(struct mlx5dv_dr_rule *rule);
 
 enum mlx5dv_dr_action_flags {
diff --git a/providers/mlx5/mlx5dv_dr.h b/providers/mlx5/mlx5dv_dr.h
index 7c7d0836..2cc8e4a1 100644
--- a/providers/mlx5/mlx5dv_dr.h
+++ b/providers/mlx5/mlx5dv_dr.h
@@ -1065,6 +1065,7 @@ struct mlx5dv_dr_domain {
 	struct dr_domain_info		info;
 	struct list_head		tbl_list;
 	uint32_t			flags;
+	int				spinlock;
 	/* protect debug lists of all tracked objects */
 	pthread_spinlock_t		debug_lock;
 	/* statistcs */
@@ -1672,6 +1673,8 @@ int dr_send_ring_alloc(struct mlx5dv_dr_domain *dmn);
 void dr_send_ring_free(struct mlx5dv_dr_domain *dmn);
 int dr_send_ring_force_drain(struct mlx5dv_dr_domain *dmn);
 bool dr_send_allow_fl(struct dr_devx_caps *caps);
+int dr_postsend_icm_data_unlocked(struct mlx5dv_dr_domain *dmn,
+				  struct postsend_info *send_info, int ring_idx);
 int dr_send_postsend_ste(struct mlx5dv_dr_domain *dmn, struct dr_ste *ste,
 			 uint8_t *data, uint16_t size, uint16_t offset,
 			 uint8_t ring_idx);
